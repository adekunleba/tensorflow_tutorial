{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN with Tensorflow Eager.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adekunleba/tensorflow_tutorial/blob/master/CNN_with_Tensorflow_Eager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UHcORc5hKEAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Learning to write Convolution Neural Networks with Tensorflow Eager Execution and Tf.keras API"
      ]
    },
    {
      "metadata": {
        "id": "QZESuKQS4gMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-b8YIY5T5N77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using MNIST Data can we do a tf.eager project to build a Model using Convolutional Neural Network.\n",
        "\n",
        "The major focus of this project is to adapt tf.eager in the process of training machine learning, right from data generation and converting the DataSet which is the format that is recommended by tensorflow to passing the data to the model and computing gradients on the data as the training goes on.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_AAsp7z342ju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3PJRlXnC5ZTh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input_data.read_data_sets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UqcY1hz8jV-4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mnist from keras datasets presents you with a way to get data first, it comes already prepared with the data and the label. In some other custom cases, you might need to define the data generator approach to get your data and read it into the dataset.\n",
        "\n",
        "However, as smooth as the data from mnist is, it is also important to note that we had to convert a numpy array which is the format which the data from mnist is, to a Dataset,\n",
        "\n",
        "Also, the labels existed as the corresponding figure, we needed to convert this to a one-hot approach with the help of `tf.one_hot` we are able to convert a numpy list of corresponding digit values to a one-hot encoding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9Lf740sCm6Gt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "8bmFk16P5dLP",
        "colab_type": "code",
        "outputId": "d7a9b9d9-a22b-4da8-e20d-8d62784dcfc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((-1, 28, 28, 1))\n",
        "x_test = x_test.reshape((-1, 28, 28, 1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fI4UCLAC6abH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_train is a numpy array of `(60000, 28, 28, 1)`"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQJ5Z4cL6nxo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#convert array to one hot with tensorflow\n",
        "num_classes = 10\n",
        "y_train = tf.one_hot(y_train, depth=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y_z6-moQ6cLt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = tf.one_hot(y_test, depth=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgFlGqukk9h3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using `tf.data.Dataset.from_tensor_slices` we can then get the numpy array of`images` and `labels` into a Dataset.\n",
        "\n",
        "*Note that we can apply many other `map` functions to the datase if we find need of it.*\n",
        "\n",
        "\n",
        "The interesting thing about dataset is also the ability to easily batch, hence on the call to `batch`, tensorflow takes care of batching the data to model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nB_SQki-8Q1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train.numpy()))\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test.numpy()))\n",
        "test_dataset = test_dataset.batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ry_fy6gFISJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using Model subclassing API, this is an imperative approach introduced into Tensorflow. It allows enough flexibility and quick changes to the architecutre.\n",
        "\n",
        "With this, you can control the approach of the forward pass of the data through your network. I have found this to be more usefull when trying out new Ideas. \n",
        "\n",
        "It's approach is quite simple, you subclass *Model* object from `tf.keras.Model`, this allows you to make a method `call()` which allows you to structure the forward pass of your architecture.\n",
        "\n",
        "Once this is done, you can handle the components of training an architure in a detached mode i.e \n",
        "\n",
        "* send the data through foward pass\n",
        "* calculate the loss on the forward pass\n",
        "* compute the gradients.\n",
        "* optimize the gradients with your defined opitimizer\n",
        "* Repeat till the end of the data"
      ]
    },
    {
      "metadata": {
        "id": "gnVKhz278xT9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Build your CNN Model\n",
        "\n",
        "class ConvBN(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, filters, size, apply_batchnorm=True, apply_pooling=False):\n",
        "    super(ConvBN, self).__init__()\n",
        "    self.apply_batchnorm = apply_batchnorm\n",
        "    self.apply_pooling = apply_pooling\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "    \n",
        "    self.conv = tf.keras.layers.Conv2D(filters, (size, size), strides=2, padding=\"same\",\n",
        "                                       kernel_initializer=initializer, use_bias=False)\n",
        "    self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "    self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    \n",
        "    \n",
        "  def call(self, inputs, training):\n",
        "    x = self.conv(inputs)\n",
        "    if self.apply_batchnorm:\n",
        "      x = self.batchnorm(x, training=training)\n",
        "    if self.apply_pooling:\n",
        "      x = self.pool(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrRGJ4MQ9T1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTCNN(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, num_classes):\n",
        "    super(MNISTCNN, self).__init__()\n",
        "    self.conv1 = ConvBN(64, 4)\n",
        "    self.conv2 = ConvBN(128, 4, apply_pooling=False)\n",
        "    self.conv3 = ConvBN(128, 4, apply_pooling=False)\n",
        "    \n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.last = tf.keras.layers.Dense(num_classes)\n",
        "  \n",
        "  @tf.contrib.eager.defun\n",
        "  def call(self, inputs, training):\n",
        "    x = self.conv1(inputs, training=training)\n",
        "    x = self.conv2(x, training=training)\n",
        "    x = self.conv3(x, training=training)\n",
        "    \n",
        "    \n",
        "    last = self.last(self.flatten(x))\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSsdeceAL0Tp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*PS: Once the model's architecture has been structured, you run keras `compile` over the model and use keras `fit` or `fit_generator`. This is an easy feat for people who are already used to the keras api. *"
      ]
    },
    {
      "metadata": {
        "id": "miR9dR9gNu6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = MNISTCNN(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oRmfMe5iIY1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.fit_generator() Tensorflow Model Subclassing also have the ability to fit on a generator."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lykM6LxvgQyf",
        "colab_type": "code",
        "outputId": "6d22de24-03c5-4c89-8d7b-51edb3f7933a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_bn (ConvBN)             multiple                  1280      \n",
            "_________________________________________________________________\n",
            "conv_bn_1 (ConvBN)           multiple                  131584    \n",
            "_________________________________________________________________\n",
            "conv_bn_2 (ConvBN)           multiple                  262656    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  20490     \n",
            "=================================================================\n",
            "Total params: 416,010\n",
            "Trainable params: 415,370\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aCeokGLOIPll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Teh"
      ]
    },
    {
      "metadata": {
        "id": "acjh48OTE9mP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "#define optimizer \n",
        "optimizer = tf.train.AdamOptimizer(2e-4, beta1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vngFVZ9VJAT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Set up checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J94Mq7xZMhlT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_loss(logits, one_hot_labels):\n",
        "  return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=one_hot_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-WXDso-d5-Hk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Write code to always see the prediction. The Test part of the deal\n",
        "\n",
        "def generate_test(model, input, target):\n",
        "  prediction = model(input, training=True)\n",
        "  #prediction is (1, 10)\n",
        "  print(\"model predicted {} while actual image is {}\".format())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEg3B9hJQTI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training the model using `Tensorflow Autograd` which is a powerful way of creating gradients in Eager Execution mode. Remember, this approach is actually to compine the keras API with the power of Eager Execution.\n",
        "\n",
        "`tf.GradientTape` is basically a way to monitor operations in a forward pass so as to easily compute gradients over those operations.\n",
        "\n",
        "`gradient` call to `tf.GradientTape` can only be made once, this is why it is done for every pass through the dataset.\n",
        "\n",
        "One can think of it like this for the whole number of epochs, \n",
        "\n",
        "* Run a forward pass over a batch of your data and compute the loss over this batch\n",
        "* Monitor the operations involved in the forward pass\n",
        "* Run a single differentiation over this\n",
        "* Optimize the gradients\n",
        "* Repeat on another batch of your data until you exhaust the number of epochs."
      ]
    },
    {
      "metadata": {
        "id": "vgrcuww7KO65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "EPOCHS = 200\n",
        "loss_history =[]\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  for input_image, target in train_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "            \n",
        "      logits = model(input_image, training=True) #Run a foward pass\n",
        "      loss = gradient_loss(logits=logits, one_hot_labels=target) #Note target was already in one-hot encoding\n",
        "      \n",
        "    #Just Added  \n",
        "    loss_history.append(loss.numpy())\n",
        "      \n",
        "    gradients = tape.gradient(loss, model.variables)\n",
        "    \n",
        "    \n",
        "    #Do Optimization\n",
        "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
        "  \n",
        "  \n",
        "  if epoch % 2 == 0:\n",
        "    for test_image, tar in test_dataset.take(1):\n",
        "      prediction = model(test_image, training=True)\n",
        "      print(\"model predicted {} while actual image is {}\".format(np.argmax(prediction), np.argmax(tar)))\n",
        "      \n",
        "  \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
        "                                                          time.time()-start))\n",
        "      \n",
        "      \n",
        "      #Tape needs Gradient.\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZK_7sfsOF2U",
        "colab_type": "code",
        "outputId": "02d16e6a-7cdb-429f-989a-0e3938e0da36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "!ls training_checkpoints/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t    ckpt-3.data-00000-of-00001\n",
            "ckpt-1.data-00000-of-00001  ckpt-3.index\n",
            "ckpt-1.index\t\t    ckpt-4.data-00000-of-00001\n",
            "ckpt-2.data-00000-of-00001  ckpt-4.index\n",
            "ckpt-2.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2SmdzhWankCv",
        "colab_type": "code",
        "outputId": "293b86b5-97d5-48f3-b240-920e3bd6c334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(np.array([0, 0, 1, 0, 0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "50G36H868Snm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}