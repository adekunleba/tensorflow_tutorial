{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adekunleba/tensorflow_tutorial/blob/master/Linear_Regression_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jYwO1Zxewa7P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with Tensorflow Eeager Execution"
      ]
    },
    {
      "metadata": {
        "id": "Pua_Hxx0t1-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waXIGL-Cxc_P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Every machine learning model application starts with establishing the data, we can make use of a synthetic data as a starting point.\n",
        "\n",
        "Also, we will also train a model on one other real data, the aim is to show the numerical computation capacity of Tensorflow."
      ]
    },
    {
      "metadata": {
        "id": "9o7yuCb1xcG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_synthetic_data(w, b, noise_level, batch_size, num_batches):\n",
        "  def batch(_):\n",
        "    x = tf.random_normal(shape=[batch_size, tf.shape(w)[0]])\n",
        "    y = tf.matmul(x, w) + b + noise_level + tf.random_normal([])\n",
        "    return x, y\n",
        "  \n",
        "  \n",
        "  #Use Cpu to do this\n",
        "  with tf.device(\"/device:CPU:0\"):\n",
        "    return tf.data.Dataset.range(num_batches).map(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zyJ0bPfEOXSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make a synthetic data using the `make_synthetic_data` function, following this, you give a base weight and bias value for you model to be optimized on.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ot_bzQU9xT5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "true_w = [[-2.0], [4.0], [1.0]]\n",
        "true_b = [0.5]\n",
        "noise_level = 0.01\n",
        "\n",
        "# Training constants.\n",
        "batch_size = 64\n",
        "learning_rate = 0.1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = make_synthetic_data(true_w, true_b, noise_level, batch_size, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9L3WJOKRsdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layers = tf.keras.layers\n",
        "\n",
        "\n",
        "model = layers.Dense(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YMPo8KmUAQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mse = lambda xs, ys: tf.reduce_mean(tf.square(tf.subtract(model(xs), ys)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPmQs9jpPB6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using `implicit_value_and_gradient` to generate a gradient function. Basically, the function takes another function as input, the input function is usually a computation that needs to undergo differentiation, say for example a *loss function*. \n",
        "\n",
        "`impliit_value_and_gradient` returns the values and gradient obtained upon differentiating an input function.\n",
        "\n",
        "A possible replacement for the `mse lambda function` above will be something like this.\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "def loss(xs, ys):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(model(xs), ys)))\n",
        " ```"
      ]
    },
    {
      "metadata": {
        "id": "RKvjj7ooUaIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfe = tf.contrib.eager\n",
        "loss_and_grads = tfe.implicit_value_and_gradients(f=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ml0i_CuRxYg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Optimizer is what helps to shape the model to it's optimum values by comparing the loss upon every forward pass through the network and updating the model parameters accordingly.\n",
        "\n",
        "One key thing to note is how loss functions and optimizers work together to give the best possible model. The results from the loss functions helps the  optimization algorithm makes it's decision on how to change the various parameter to tend it towards the most accurate.\n",
        "\n",
        "Usually depicted with the hiker's approach to find the lowest loss.\n",
        "\n",
        "Hence you can start to thing of algorithms such as `gradient descent` in it's various forms i.e `stochastic` and `mini-batch`, others like `Adagrad`, `RMSprop` and `Adam` optimizers."
      ]
    },
    {
      "metadata": {
        "id": "v6N_1mbCaA3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Build Optimizer\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17s9G1puZk0e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "07e2da09-d6d8-4132-b493-0631ca5456b9"
      },
      "cell_type": "code",
      "source": [
        "for i, (xs, ys) in enumerate(tfe.Iterator(data)):\n",
        "  loss, grads = loss_and_grads(xs, ys)\n",
        "  print(\"Iteration %d: loss = %s\" % (i, loss.numpy()))\n",
        "  \n",
        "  optimizer.apply_gradients(grads)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0: loss = 15.084289\n",
            "Iteration 1: loss = 11.133729\n",
            "Iteration 2: loss = 11.7282505\n",
            "Iteration 3: loss = 9.82547\n",
            "Iteration 4: loss = 6.488843\n",
            "Iteration 5: loss = 3.9334645\n",
            "Iteration 6: loss = 1.4190112\n",
            "Iteration 7: loss = 1.7760332\n",
            "Iteration 8: loss = 2.648704\n",
            "Iteration 9: loss = 0.56297696\n",
            "Iteration 10: loss = 0.7839244\n",
            "Iteration 11: loss = 0.26919875\n",
            "Iteration 12: loss = 0.10689156\n",
            "Iteration 13: loss = 2.5968251\n",
            "Iteration 14: loss = 5.1002445\n",
            "Iteration 15: loss = 2.6390283\n",
            "Iteration 16: loss = 2.118974\n",
            "Iteration 17: loss = 0.8578691\n",
            "Iteration 18: loss = 0.059626978\n",
            "Iteration 19: loss = 0.12894574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nxY6CGtLNo79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "bbb953d8-3d13-4512-ce41-e4e1afe3227a"
      },
      "cell_type": "code",
      "source": [
        "print(\"Model Weight After Training \\n {}\".format(model.variables[0].numpy()))\n",
        "\n",
        "print(\"Model Bias After Training \\n {}\".format(model.variables[1].numpy()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Weight After Training \n",
            " [[-1.9116454]\n",
            " [ 3.902374 ]\n",
            " [ 1.0100844]]\n",
            "Model Bias After Training \n",
            " [0.72644985]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K3C4YmGU07NL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***This is a later approach - Do functional approach first***\n",
        "\n",
        "Since tensorflow uses keras model out of the box, hence our model will use the keras way of building custom models.\n",
        "\n",
        "Hence we can structure our codes in a more organized approach.\n",
        "\n",
        "\n",
        "There are basic 3 components for your custom model when using Tensorflow Eager Execution:\n",
        "\n",
        "\n",
        "* init Method: Define your various networks.\n",
        "* call method: Invokes the model, this is where the layer logic all resides. Most times, the sequential approach to modelling Keras models actually happen here.\n",
        "* fit method: Method fit assembled model on data.\n",
        "* predict method: Method predicts on new data.\n",
        "\n",
        "***Other Utility Functions***\n",
        "\n",
        "* Save model method\n",
        "* compute_accuracy\n",
        "* restore_model\n"
      ]
    },
    {
      "metadata": {
        "id": "G9Cdecc8zk3J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(LinearRegressionModel, self).__init__()\n",
        "    \n",
        "    #Construct a single dense layer which is actually a linear layer.\n",
        "    self._layer = layers.Dense(1) #Creates model as seen above\n",
        "    \n",
        "    \n",
        "  def call(self, xs):\n",
        "    \n",
        "    return self._layer(xs) # check up it's same thing as model(xs)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "def fit(model, dataset, optimizer, verbose=False, logdir=None):\n",
        "\n",
        "  ''' Defines the model activity on a dataset\n",
        "\n",
        "  Args:\n",
        "\n",
        "    model: A Tensorflow-Keras to fit\n",
        "    dataset: the tf.data.Dataset to use as training data\n",
        "    optimizer: The tensorflow optimizer to be used for model update\n",
        "  '''\n",
        "\n",
        "  #Check out the use of self.call here\n",
        "  mse = lambda xs, ys: tf.reduce_mean(tf.square(tf.subtract(model(xs), ys)))\n",
        "  loss_and_grads = tfe.implicit_value_and_gradients(mse)\n",
        "\n",
        "\n",
        "  '''Make Summary Writer'''\n",
        "  if logdir:\n",
        "    summary_writer = tf.contrib.summary.create_file_writer(logdir)\n",
        "\n",
        "\n",
        "  '''Train and get loss and grads on dataset'''\n",
        "  for i, (xs, ys) in enumerate(tfe.Iterator(dataset)):\n",
        "    loss, grads = loss_and_grads(xs, ys)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Iteration %d: loss = %s\" % (i, loss.numpy()))\n",
        "\n",
        "    '''Apply optimizer on gradients for every run over the dataset'''\n",
        "    optimizer.apply_gradients(grads)\n",
        "\n",
        "\n",
        "    '''Write to summary-writer'''\n",
        "    if logdir:\n",
        "      with summary_writer.as_default():\n",
        "        with tf.contrib.summary.always_record_summaries():\n",
        "          tf.contrib.summary.scala(\"loss\", loss, step=i)\n",
        "          tf.contrib.summary.scala(\"step\", i, step= i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNXPEmOydiPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vC0VcCI20nHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "e6052deb-9416-48c9-9c2c-1a80e8f14031"
      },
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel()\n",
        "fit(model, data, optimizer,True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0: loss = 16.548885\n",
            "Iteration 1: loss = 8.613136\n",
            "Iteration 2: loss = 6.946666\n",
            "Iteration 3: loss = 5.441123\n",
            "Iteration 4: loss = 5.062219\n",
            "Iteration 5: loss = 2.5306613\n",
            "Iteration 6: loss = 3.2678819\n",
            "Iteration 7: loss = 2.1243205\n",
            "Iteration 8: loss = 0.801169\n",
            "Iteration 9: loss = 2.3897\n",
            "Iteration 10: loss = 0.5184775\n",
            "Iteration 11: loss = 0.8828287\n",
            "Iteration 12: loss = 1.2366993\n",
            "Iteration 13: loss = 0.11105306\n",
            "Iteration 14: loss = 0.27638763\n",
            "Iteration 15: loss = 0.43359774\n",
            "Iteration 16: loss = 1.6443429\n",
            "Iteration 17: loss = 2.5629683\n",
            "Iteration 18: loss = 0.20214197\n",
            "Iteration 19: loss = 1.02789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gKQ6penhi2xM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}